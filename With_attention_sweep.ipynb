{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahistaAfreen/DL_DA6401_A3/blob/main/With_attention_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vkvje2XA3r"
      },
      "source": [
        "# Import package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:45.220189Z",
          "iopub.status.busy": "2022-05-13T17:56:45.219611Z",
          "iopub.status.idle": "2022-05-13T17:56:59.642417Z",
          "shell.execute_reply": "2022-05-13T17:56:59.641258Z",
          "shell.execute_reply.started": "2022-05-13T17:56:45.220142Z"
        },
        "id": "u0qCvgqBxzLo",
        "outputId": "ef72e66a-769c-403d-ed18-ddfb84a91456",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: xtarfile in /usr/local/lib/python3.11/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install xtarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.646057Z",
          "iopub.status.busy": "2022-05-13T17:56:59.645631Z",
          "iopub.status.idle": "2022-05-13T17:56:59.654209Z",
          "shell.execute_reply": "2022-05-13T17:56:59.653024Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.646002Z"
        },
        "id": "6xAqFEssB1RJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:56:59.661908Z",
          "iopub.status.busy": "2022-05-13T17:56:59.659308Z",
          "iopub.status.idle": "2022-05-13T17:57:07.051305Z",
          "shell.execute_reply": "2022-05-13T17:57:07.050263Z",
          "shell.execute_reply.started": "2022-05-13T17:56:59.661854Z"
        },
        "id": "s6xB0KDuy95e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from collections import Counter\n",
        "import os\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ8lIJvgitRW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from os.path import exists\n",
        "import xtarfile as tarfile\n",
        "import pandas as pd\n",
        "import keras\n",
        "START_TOKEN=\"0\"\n",
        "END_TOKEN=\"1\"\n",
        "\n",
        "\n",
        "\"\"\"Download dataset if not exists\"\"\"\n",
        "def downloadDataSet():\n",
        "   cwd = os.getcwd()\n",
        "\n",
        "   file_exists = exists('./dakshina_dataset_v1.0.tar')\n",
        "   if(file_exists==False):\n",
        "     print('downloading....')\n",
        "     os.system('curl -SL https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar > dakshina_dataset_v1.0.tar')\n",
        "     print('download Complete')\n",
        "   extract_exists = exists('./dakshina_dataset_v1.0/')\n",
        "   if(extract_exists==False):\n",
        "     print('Extracting..')\n",
        "     with tarfile.open('dakshina_dataset_v1.0.tar', 'r') as archive:\n",
        "         archive.extractall()\n",
        "     print('Complete')\n",
        "   print('You are all set')\n",
        "def get_files(language):\n",
        "\n",
        "  train_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.train.tsv'\n",
        "  val_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.dev.tsv'\n",
        "  test_dir='./dakshina_dataset_v1.0/'+language+'/lexicons/'+language+'.translit.sampled.test.tsv'\n",
        "\n",
        "  return train_dir, val_dir, test_dir\n",
        "\n",
        "\"\"\"Generate Tokens\"\"\"\n",
        "def tokenize(lang,tokenizer=None):\n",
        "    \"\"\" Uses tf.keras tokenizer to tokenize the data/words into characters\n",
        "    \"\"\"\n",
        "    if(tokenizer==None):\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "        tokenizer.fit_on_texts(lang)\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                            padding='post')\n",
        "    else:\n",
        "\n",
        "        lang_tensor = tokenizer.texts_to_sequences(lang)\n",
        "        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,\n",
        "                                                        padding='post')\n",
        "\n",
        "    return lang_tensor, tokenizer\n",
        "\n",
        "\n",
        "\"\"\"Preprocessing the data\"\"\"\n",
        "def preprocess_data(fpath,ip_tokenizer=None, tgt_tokenizer=None):\n",
        "\n",
        "    #Read data from files\n",
        "    df = pd.read_csv(fpath, sep=\"\\t\", header=None)\n",
        "\n",
        "    #Add start and end token\n",
        "    df[0] = df[0].apply( lambda x:START_TOKEN+x+END_TOKEN)\n",
        "    ip_tensor, ip_tokenizer = tokenize(df[1].astype(str).tolist(), tokenizer=ip_tokenizer)\n",
        "\n",
        "    tgt_tensor, tgt_tokenizer = tokenize(df[0].astype(str).tolist(), tokenizer=tgt_tokenizer)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((ip_tensor, tgt_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "\n",
        "    return dataset, ip_tokenizer, tgt_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bahdanau Attention**"
      ],
      "metadata": {
        "id": "0NRptwepnCOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.053772Z",
          "iopub.status.busy": "2022-05-13T17:57:07.053448Z",
          "iopub.status.idle": "2022-05-13T17:57:07.068478Z",
          "shell.execute_reply": "2022-05-13T17:57:07.066675Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.053724Z"
        },
        "id": "d1J0s1-UsYaC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Bahdanau Attention\n",
        "\"\"\"\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    #Define weights\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, enc_state, enc_out):\n",
        "\n",
        "    enc_state = tf.concat(enc_state, 1)\n",
        "    enc_state = tf.expand_dims(enc_state, 1)\n",
        "\n",
        "    #Calculate attention scores\n",
        "    score = self.V(tf.nn.tanh(self.W1(enc_state) + self.W2(enc_out)))\n",
        "\n",
        "    #Fetch Attention weights\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * enc_out\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.07297Z",
          "iopub.status.busy": "2022-05-13T17:57:07.072345Z",
          "iopub.status.idle": "2022-05-13T17:57:07.906438Z",
          "shell.execute_reply": "2022-05-13T17:57:07.905226Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.072904Z"
        },
        "id": "ls7v013HipOJ",
        "outputId": "a3259ece-9fdb-4bce-c122-1642b8dddb98",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.935325Z",
          "iopub.status.busy": "2022-05-13T17:57:07.93476Z",
          "iopub.status.idle": "2022-05-13T17:57:07.95351Z",
          "shell.execute_reply": "2022-05-13T17:57:07.952408Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.935278Z"
        },
        "id": "ty-yWidXsJyD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Parameters():\n",
        "  def  __init__(self,  language='te',encoder_layers=1,decoder_layers=1,embedding_dim=128,\\\n",
        "                layer_type='lstm', units=128, dropout=0.5, attention=False,attention_type=\"Luong\",batch_size=128,\\\n",
        "                apply_beam_search=False,apply_teacher_forcing=False,teacher_forcing_ratio=1,\\\n",
        "                 save_outputs=None,epochs=5,wandb=None,beamWidth=5,restoreBestModel=True,\\\n",
        "                 patience=2,encoder_vocab_size=64,decoder_vocab_size=64):\n",
        "        self.language = language\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_layers=encoder_layers\n",
        "        self.decoder_layers=decoder_layers\n",
        "        self.layer_type = layer_type\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.stats = []\n",
        "        self.wandb=wandb\n",
        "        self.epochs=epochs\n",
        "        self.batch_size = 128\n",
        "        self.apply_beam_search = apply_beam_search\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing=apply_teacher_forcing\n",
        "        self.save_outputs=save_outputs\n",
        "        self.restoreBestModel=restoreBestModel\n",
        "        self.attention_type=attention_type\n",
        "        self.patience=patience\n",
        "        self.encoder_vocab_size=encoder_vocab_size\n",
        "        self.decoder_vocab_size=decoder_vocab_size\n",
        "        self.teacher_forcing_ratio=teacher_forcing_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder**"
      ],
      "metadata": {
        "id": "yGRobVz_nX5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.956076Z",
          "iopub.status.busy": "2022-05-13T17:57:07.955483Z",
          "iopub.status.idle": "2022-05-13T17:57:07.97555Z",
          "shell.execute_reply": "2022-05-13T17:57:07.97435Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.956029Z"
        },
        "id": "KQSzzTVAsEX0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Helper function for creating RNN layers\n",
        "def get_layer(layer_type, units, dropout, return_sequences, return_state):\n",
        "    if layer_type.lower() == \"lstm\":\n",
        "        return tf.keras.layers.LSTM(units,\n",
        "                                    return_sequences=return_sequences,\n",
        "                                    return_state=return_state,\n",
        "                                    dropout=dropout)\n",
        "    elif layer_type.lower() == \"gru\":\n",
        "        return tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=return_sequences,\n",
        "                                   return_state=return_state,\n",
        "                                   dropout=dropout)\n",
        "    elif layer_type.lower() == \"rnn\":\n",
        "        return tf.keras.layers.SimpleRNN(units,\n",
        "                                   return_sequences=return_sequences,\n",
        "                                   return_state=return_state,\n",
        "                                   dropout=dropout)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported layer type: {layer_type}\")\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, param):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer_type = param.layer_type.lower()\n",
        "        self.n_layers = param.encoder_layers\n",
        "        self.units = param.units\n",
        "        self.dropout = param.dropout\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            param.encoder_vocab_size,\n",
        "            param.embedding_dim,\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        # Create RNN layers directly in __init__\n",
        "        self.rnn_layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            # For stacked RNNs, all layers except last should return sequences\n",
        "            return_sequences = True if i < self.n_layers - 1 else True  # Always return sequences for encoder\n",
        "            layer = get_layer(\n",
        "                self.layer_type,\n",
        "                self.units,\n",
        "                self.dropout,\n",
        "                return_sequences=return_sequences,\n",
        "                return_state=True\n",
        "            )\n",
        "            self.rnn_layers.append(layer)\n",
        "\n",
        "        # Add this to ensure the layer is properly built\n",
        "        self.built = True\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, hidden=None):\n",
        "        # x: [batch_size, seq_len]\n",
        "        batch_size = tf.shape(x)[0]\n",
        "\n",
        "        # Apply embedding\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # Initialize hidden states if not provided\n",
        "        if hidden is None:\n",
        "            if self.layer_type == \"lstm\":\n",
        "                hidden = []\n",
        "                for _ in range(self.n_layers):\n",
        "                    h = tf.zeros((batch_size, self.units))\n",
        "                    c = tf.zeros((batch_size, self.units))\n",
        "                    hidden.extend([h, c])\n",
        "            else:  # GRU or RNN\n",
        "                hidden = [tf.zeros((batch_size, self.units)) for _ in range(self.n_layers)]\n",
        "\n",
        "        outputs = []\n",
        "        states = []\n",
        "\n",
        "        # Format the hidden states based on RNN type\n",
        "        if self.layer_type == \"lstm\":\n",
        "            hidden_states = []\n",
        "            for i in range(self.n_layers):\n",
        "                if 2*i+1 < len(hidden):\n",
        "                    hidden_states.append([hidden[2*i], hidden[2*i+1]])\n",
        "                else:\n",
        "                    # Default to None if not enough states\n",
        "                    hidden_states.append(None)\n",
        "        else:  # GRU or RNN\n",
        "            hidden_states = []\n",
        "            for i in range(self.n_layers):\n",
        "                if i < len(hidden):\n",
        "                    hidden_states.append(hidden[i])\n",
        "                else:\n",
        "                    hidden_states.append(None)\n",
        "\n",
        "        # Process through RNN layers\n",
        "        current_input = x\n",
        "\n",
        "        for i, rnn in enumerate(self.rnn_layers):\n",
        "            if hidden_states[i] is not None:\n",
        "                if self.layer_type == \"lstm\":\n",
        "                    # For LSTM with initial state\n",
        "                    output, h_state, c_state = rnn(current_input, initial_state=hidden_states[i])\n",
        "                    states.extend([h_state, c_state])\n",
        "                else:\n",
        "                    # For GRU/RNN with initial state\n",
        "                    output, state = rnn(current_input, initial_state=hidden_states[i])\n",
        "                    states.append(state)\n",
        "            else:\n",
        "                # No initial state provided\n",
        "                if self.layer_type == \"lstm\":\n",
        "                    output, h_state, c_state = rnn(current_input)\n",
        "                    states.extend([h_state, c_state])\n",
        "                else:\n",
        "                    output, state = rnn(current_input)\n",
        "                    states.append(state)\n",
        "\n",
        "            current_input = output\n",
        "            outputs.append(output)\n",
        "\n",
        "        # Return the output of last RNN layer and all states\n",
        "        return outputs[-1], states\n",
        "\n",
        "    def initialize_hidden_state(self, batch_size):\n",
        "        if self.layer_type == \"lstm\":\n",
        "            # For LSTM, we need h_state and c_state for each layer\n",
        "            states = []\n",
        "            for _ in range(self.n_layers):\n",
        "                h = tf.zeros((batch_size, self.units))\n",
        "                c = tf.zeros((batch_size, self.units))\n",
        "                states.extend([h, c])\n",
        "        else:\n",
        "            # For GRU/RNN, we need one state per layer\n",
        "            states = [tf.zeros((batch_size, self.units)) for _ in range(self.n_layers)]\n",
        "\n",
        "        return states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder**"
      ],
      "metadata": {
        "id": "zBSHDVmgna_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:07.981314Z",
          "iopub.status.busy": "2022-05-13T17:57:07.980762Z",
          "iopub.status.idle": "2022-05-13T17:57:08.004816Z",
          "shell.execute_reply": "2022-05-13T17:57:08.003792Z",
          "shell.execute_reply.started": "2022-05-13T17:57:07.981262Z"
        },
        "id": "NQHJ8JtysOCX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Bahdanau Attention mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, hidden, enc_output):\n",
        "        # hidden: [batch_size, hidden_size]\n",
        "        # enc_output: [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Add time axis to hidden state\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(enc_output) + self.W2(hidden_with_time_axis)\n",
        "        ))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Calculate context vector\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Helper function for creating RNN layers (same as in Encoder)\n",
        "def get_layer(layer_type, units, dropout, return_sequences, return_state):\n",
        "    if layer_type.lower() == \"lstm\":\n",
        "        return tf.keras.layers.LSTM(units,\n",
        "                                    return_sequences=return_sequences,\n",
        "                                    return_state=return_state,\n",
        "                                    dropout=dropout)\n",
        "    elif layer_type.lower() == \"gru\":\n",
        "        return tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=return_sequences,\n",
        "                                   return_state=return_state,\n",
        "                                   dropout=dropout)\n",
        "    elif layer_type.lower() == \"rnn\":\n",
        "        return tf.keras.layers.SimpleRNN(units,\n",
        "                                   return_sequences=return_sequences,\n",
        "                                   return_state=return_state,\n",
        "                                   dropout=dropout)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported RNN type: {layer_type}\")\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, param):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layer_type = param.layer_type.lower()\n",
        "        self.n_layers = param.decoder_layers\n",
        "        self.units = param.units\n",
        "        self.dropout = param.dropout\n",
        "        self.attention = param.attention\n",
        "        self.attention_type = getattr(param, 'attention_type', 'bahdanau')\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding_layer = layers.Embedding(\n",
        "            input_dim=param.decoder_vocab_size,\n",
        "            output_dim=param.embedding_dim,\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        # Output dense layer\n",
        "        self.dense = layers.Dense(param.decoder_vocab_size, activation=\"softmax\")\n",
        "\n",
        "        # Attention mechanism\n",
        "        if self.attention:\n",
        "            self.attention_layer = BahdanauAttention(self.units)\n",
        "\n",
        "        # Create RNN layers directly in __init__\n",
        "        self.rnn_layers = []\n",
        "        for i in range(self.n_layers):\n",
        "            return_sequences = True if i < self.n_layers - 1 else False\n",
        "            layer = get_layer(\n",
        "                self.layer_type, self.units, self.dropout,\n",
        "                return_sequences=return_sequences, return_state=True\n",
        "            )\n",
        "            self.rnn_layers.append(layer)\n",
        "\n",
        "        # Add this to ensure the layer is properly built\n",
        "        self.built = True\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, hidden, enc_out=None):\n",
        "        # x: [batch_size, 1]\n",
        "        # hidden: list of states from encoder\n",
        "        # enc_out: encoder outputs for attention\n",
        "\n",
        "        # Apply embedding\n",
        "        x = self.embedding_layer(x)  # [batch_size, 1, embedding_dim]\n",
        "\n",
        "        attention_weights = None\n",
        "\n",
        "        # Format hidden states based on RNN type\n",
        "        if self.layer_type == \"lstm\":\n",
        "            # For LSTM, we need h_state and c_state for each layer\n",
        "            hidden_states = []\n",
        "            for i in range(self.n_layers):\n",
        "                if 2*i+1 < len(hidden):\n",
        "                    hidden_states.append([hidden[2*i], hidden[2*i+1]])\n",
        "                else:\n",
        "                    hidden_states.append(None)\n",
        "\n",
        "            # Use first hidden state for attention\n",
        "            attention_state = hidden[0] if hidden else None\n",
        "        else:\n",
        "            # For GRU/RNN, we need one state per layer\n",
        "            hidden_states = []\n",
        "            for i in range(self.n_layers):\n",
        "                if i < len(hidden):\n",
        "                    hidden_states.append(hidden[i])\n",
        "                else:\n",
        "                    hidden_states.append(None)\n",
        "\n",
        "            # Use first hidden state for attention\n",
        "            attention_state = hidden[0] if hidden else None\n",
        "\n",
        "        # Apply attention if enabled\n",
        "        if self.attention and enc_out is not None and attention_state is not None:\n",
        "            # Apply attention mechanism\n",
        "            context_vector, attention_weights = self.attention_layer(attention_state, enc_out)\n",
        "\n",
        "            # Expand context vector to match x's time dimension\n",
        "            context_vector_expanded = tf.expand_dims(context_vector, 1)\n",
        "\n",
        "            # Concatenate context vector with input embedding along feature dimension\n",
        "            x = tf.concat([context_vector_expanded, x], axis=-1)\n",
        "\n",
        "        # Process through RNN layers\n",
        "        states = []\n",
        "        current_input = x\n",
        "\n",
        "        for i, rnn in enumerate(self.rnn_layers):\n",
        "            if hidden_states[i] is not None:\n",
        "                if self.layer_type == \"lstm\":\n",
        "                    # For LSTM with initial state\n",
        "                    output, h_state, c_state = rnn(current_input, initial_state=hidden_states[i])\n",
        "                    states.extend([h_state, c_state])\n",
        "                else:\n",
        "                    # For GRU/RNN with initial state\n",
        "                    output, state = rnn(current_input, initial_state=hidden_states[i])\n",
        "                    states.append(state)\n",
        "            else:\n",
        "                # No initial state provided\n",
        "                if self.layer_type == \"lstm\":\n",
        "                    output, h_state, c_state = rnn(current_input)\n",
        "                    states.extend([h_state, c_state])\n",
        "                else:\n",
        "                    output, state = rnn(current_input)\n",
        "                    states.append(state)\n",
        "\n",
        "            current_input = output\n",
        "\n",
        "        # Apply the dense layer to get output probabilities\n",
        "        # If the last RNN layer returns sequences, take only the last timestep\n",
        "        if len(output.shape) > 2:\n",
        "            output = output[:, -1, :]\n",
        "\n",
        "        # Get final output prediction\n",
        "        prediction = self.dense(output)\n",
        "\n",
        "        return prediction, states, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SequenceTOSequence**"
      ],
      "metadata": {
        "id": "FlqAZqpankTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.007545Z",
          "iopub.status.busy": "2022-05-13T17:57:08.007003Z",
          "iopub.status.idle": "2022-05-13T17:57:08.101039Z",
          "shell.execute_reply": "2022-05-13T17:57:08.100005Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.007496Z"
        },
        "id": "Pa0XPzaFsSkW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SequenceTOSequence():\n",
        "    def __init__(self, parameters):\n",
        "        #Basic configurations\n",
        "        self.param = parameters\n",
        "        self.embedding_dim = parameters.embedding_dim\n",
        "        self.encoder_layers = parameters.encoder_layers\n",
        "        self.decoder_layers = parameters.decoder_layers\n",
        "        self.layer_type = parameters.layer_type\n",
        "        self.units = parameters.units\n",
        "        self.dropout = parameters.dropout\n",
        "        self.batch_size = parameters.batch_size\n",
        "\n",
        "        #Add information regarding attention layer\n",
        "        self.attention = parameters.attention\n",
        "        self.attention_type = parameters.attention_type\n",
        "\n",
        "        self.stats = []\n",
        "\n",
        "        self.apply_beam_search = parameters.apply_beam_search\n",
        "\n",
        "        #Early stop conditions\n",
        "        self.patience = parameters.patience\n",
        "        self.restoreBestModel = parameters.restoreBestModel\n",
        "\n",
        "        #teacher forcing\n",
        "        self.apply_teacher_forcing = parameters.apply_teacher_forcing\n",
        "        self.teacher_forcing_ratio = parameters.teacher_forcing_ratio\n",
        "\n",
        "    #Build model Add specific optimizers\n",
        "    def build(self, loss, metric, optimizer='adam', lr=0.001):\n",
        "        self.loss = loss\n",
        "\n",
        "        #Select specific optimizer\n",
        "        if(optimizer=='adam'):\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        if(optimizer=='nadam'):\n",
        "            self.optimizer = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
        "        else:\n",
        "            self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "\n",
        "        self.metric = metric\n",
        "\n",
        "    def set_vocabulary(self, input_tokenizer, targ_tokenizer):\n",
        "        self.input_tokenizer = input_tokenizer\n",
        "        self.targ_tokenizer = targ_tokenizer\n",
        "        self.create_model()\n",
        "\n",
        "    \"\"\"This procedure used to define Encoder Decoder Layer\"\"\"\n",
        "    def create_model(self):\n",
        "        encoder_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
        "        decoder_vocab_size = len(self.targ_tokenizer.word_index) + 1\n",
        "        self.param.encoder_vocab_size = encoder_vocab_size\n",
        "        self.param.decoder_vocab_size = decoder_vocab_size\n",
        "\n",
        "        #Add Encoder layer\n",
        "        self.encoder = Encoder(self.param)\n",
        "\n",
        "        #Create decode with or without any attention layer\n",
        "        #Check following properties to add attention\n",
        "        # self.attention\n",
        "        # self.attention_type\n",
        "        self.decoder = Decoder(self.param)\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, input, target, enc_state):\n",
        "        \"\"\"\n",
        "        Training step function with gradient tape.\n",
        "        Handles both teacher forcing and non-teacher forcing modes.\n",
        "\n",
        "        Args:\n",
        "            input: Input tensor of shape [batch_size, max_input_len]\n",
        "            target: Target tensor of shape [batch_size, max_target_len]\n",
        "            enc_state: Initial encoder state\n",
        "\n",
        "        Returns:\n",
        "            batch_loss: Average loss for this batch\n",
        "            accuracy: Current accuracy metric value\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Run input through encoder\n",
        "            enc_out, enc_state = self.encoder(input, enc_state)\n",
        "\n",
        "            # Set initial state of decoder from encoder state\n",
        "            dec_state = enc_state\n",
        "\n",
        "            # Start token for all sequences in the batch\n",
        "            dec_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]] * self.batch_size, 1)\n",
        "\n",
        "            # Determine whether to use teacher forcing for this batch\n",
        "            apply_teacher_forcing = False\n",
        "            if self.apply_teacher_forcing and random.random() < self.teacher_forcing_ratio:\n",
        "                apply_teacher_forcing = True\n",
        "\n",
        "            # Teacher forcing: use actual target tokens as next input\n",
        "            if apply_teacher_forcing:\n",
        "                for t in range(1, target.shape[1]):\n",
        "                    # Pass the decoder input, state, and encoder output to the decoder\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "                    # Calculate loss and update metrics\n",
        "                    loss += self.loss(target[:, t], preds)\n",
        "                    self.metric.update_state(target[:, t], preds)\n",
        "\n",
        "                    # Use the actual target as the next decoder input (teacher forcing)\n",
        "                    dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "            # No teacher forcing: use model's own predictions as next input\n",
        "            else:\n",
        "                for t in range(1, target.shape[1]):\n",
        "                    # Pass the decoder input, state, and encoder output to the decoder\n",
        "                    preds, dec_state, _ = self.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "                    # Calculate loss and update metrics\n",
        "                    loss += self.loss(target[:, t], preds)\n",
        "                    self.metric.update_state(target[:, t], preds)\n",
        "\n",
        "                    # Use our own prediction as the next decoder input\n",
        "                    predicted_ids = tf.argmax(preds, axis=1)\n",
        "                    dec_input = tf.expand_dims(predicted_ids, 1)\n",
        "\n",
        "            # Calculate average loss per time step\n",
        "            batch_loss = loss / tf.cast(target.shape[1], dtype=tf.float32)\n",
        "\n",
        "            # Get all trainable variables and apply gradients\n",
        "            variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    def fit(self, dataset, val_dataset, batch_size=128, epochs=5, wandb=None, apply_teacher_forcing=True, teacher_forcing_ratio=0.7):\n",
        "        self.batch_size = batch_size\n",
        "        self.apply_teacher_forcing = apply_teacher_forcing\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        #Prepare chunk of data based on batch size provided\n",
        "        steps_per_epoch = len(dataset) // self.batch_size\n",
        "        #steps_per_epoch_val = len(val_dataset) // self.batch_size\n",
        "\n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=False)\n",
        "        #val_dataset = val_dataset.batch(self.batch_size, drop_remainder=False)\n",
        "\n",
        "        sample_inp, sample_targ = next(iter(dataset))\n",
        "        self.max_target_len = sample_targ.shape[1]\n",
        "        self.max_input_len = sample_inp.shape[1]\n",
        "\n",
        "        #Store Encoder, decoder details in case model get good accuracy\n",
        "        #Will be useful to restore best model\n",
        "        self.bestEncoder = self.encoder\n",
        "        self.bestDecoder = self.decoder\n",
        "        self.bestoptimizer = self.optimizer\n",
        "\n",
        "        accuracyDegradePatience = 0\n",
        "        self.oldaccuracy = 0\n",
        "        for epoch in tqdm(range(1, epochs+1), total=epochs, desc=\"Epochs \"):\n",
        "            if(accuracyDegradePatience >= self.patience):\n",
        "                if(self.restoreBestModel == True):\n",
        "                    self.encoder = self.bestEncoder\n",
        "                    self.decoder = self.bestDecoder\n",
        "                    self.optimizer = self.bestoptimizer\n",
        "                break\n",
        "\n",
        "            ## Training loop ##\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            self.metric.reset_state()\n",
        "\n",
        "            starting_time = time.time()\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            for batch, (input, target) in enumerate(dataset.take(steps_per_epoch)):\n",
        "                #Accumulate loss and accuracy for each batch\n",
        "                batch_loss, acc = self.train(input, target, enc_state)\n",
        "                total_loss += batch_loss\n",
        "                total_acc += acc\n",
        "\n",
        "            #Calculate validation accuracy for current Epoch\n",
        "            avg_acc = total_acc / steps_per_epoch\n",
        "            avg_loss = total_loss / steps_per_epoch\n",
        "\n",
        "            # Validation loop ##\n",
        "            total_val_loss = 0\n",
        "            total_val_acc = 0\n",
        "            self.metric.reset_state()\n",
        "\n",
        "            enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "            #Process data in batches\n",
        "            avg_val_loss, avg_val_acc = self.evaluate(val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "            #Verify if model performance degrading\n",
        "            #In case train accuracy improved but no significant improvement in validation\n",
        "            #Add condition for early stopping\n",
        "            #Restore best model based on the input\n",
        "            if(self.oldaccuracy > avg_val_acc):\n",
        "                accuracyDegradePatience += 1\n",
        "            else:\n",
        "                self.bestEncoder = self.encoder\n",
        "                self.bestDecoder = self.decoder\n",
        "                self.bestoptimizer = self.optimizer\n",
        "                self.oldaccuracy = avg_val_acc\n",
        "                accuracyDegradePatience = 0\n",
        "\n",
        "            print(\"\\nTrain Loss: {0:.4f} Train Accuracy: {1:.4f} Validation Loss: {2:.4f} Validation Accuracy: {3:.4f}\".format(\n",
        "                avg_loss, avg_acc*100, avg_val_loss, avg_val_acc*100))\n",
        "\n",
        "            time_taken = time.time() - starting_time\n",
        "\n",
        "            #Add logs for WanDb\n",
        "            self.stats.append({\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": avg_loss,\n",
        "                \"val_loss\": avg_val_loss,\n",
        "                \"train_acc\": avg_acc*100,\n",
        "                \"val_acc\": avg_val_acc*100,\n",
        "                \"training time\": time_taken\n",
        "            })\n",
        "\n",
        "            #Log to wanDB\n",
        "            if not (wandb is None):\n",
        "                wandb.log(self.stats[-1])\n",
        "\n",
        "            print(f\"\\nTime taken for the epoch {time_taken:.4f}\")\n",
        "\n",
        "        print(\"\\nModel trained successfully !!\")\n",
        "\n",
        "    @tf.function\n",
        "    def validation(self, inp, trgt, encoder_state):\n",
        "        \"\"\"\n",
        "        Validation step function.\n",
        "        Always uses the model's predictions as the next input (no teacher forcing).\n",
        "\n",
        "        Args:\n",
        "            inp: Input tensor of shape [batch_size, max_input_len]\n",
        "            trgt: Target tensor of shape [batch_size, max_target_len]\n",
        "            encoder_state: Initial encoder state\n",
        "\n",
        "        Returns:\n",
        "            batch_loss: Average loss for this batch\n",
        "            accuracy: Current accuracy metric value\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "\n",
        "        # Run input through encoder\n",
        "        encoder_output, encoder_state = self.encoder(inp, encoder_state)\n",
        "\n",
        "        # Set initial state of decoder from encoder state\n",
        "        decoder_state = encoder_state\n",
        "\n",
        "        # Start token for all sequences in the batch\n",
        "        decoder_input = tf.expand_dims([self.targ_tokenizer.word_index[\"\\t\"]] * self.batch_size, 1)\n",
        "\n",
        "        # Process each time step\n",
        "        for t in range(1, trgt.shape[1]):\n",
        "            # Get decoder prediction\n",
        "            prediction, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_output)\n",
        "\n",
        "            # Calculate loss and update metrics\n",
        "            loss += self.loss(trgt[:, t], prediction)\n",
        "            self.metric.update_state(trgt[:, t], prediction)\n",
        "\n",
        "            # Use our own prediction as the next decoder input\n",
        "            predicted_ids = tf.argmax(prediction, axis=1)\n",
        "            decoder_input = tf.expand_dims(predicted_ids, 1)\n",
        "\n",
        "        # Calculate average loss per time step\n",
        "        batch_loss = loss / tf.cast(trgt.shape[1], dtype=tf.float32)\n",
        "\n",
        "        return batch_loss, self.metric.result()\n",
        "\n",
        "    def evaluate(self, test_dataset, batch_size=None):\n",
        "        \"\"\"Evaluate our model on test data\"\"\"\n",
        "        if batch_size is not None:\n",
        "            self.batch_size = batch_size\n",
        "\n",
        "        #prepare chunk of data based on the batch size\n",
        "        steps_per_epoch_test = len(test_dataset) // batch_size\n",
        "        test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "        total_test_loss = 0\n",
        "        total_test_acc = 0\n",
        "        self.metric.reset_state()\n",
        "\n",
        "        enc_state = self.encoder.initialize_hidden_state(self.batch_size)\n",
        "\n",
        "        #print(\"\\nRunning test dataset through the model...\\n\")\n",
        "        #Run in batches based on the input batch size\n",
        "        for batch, (input, target) in enumerate(test_dataset.take(steps_per_epoch_test)):\n",
        "            batch_loss, acc = self.validation(input, target, enc_state)\n",
        "            total_test_loss += batch_loss\n",
        "            total_test_acc += acc\n",
        "\n",
        "        #Calculate average test accuracy and loss\n",
        "        avg_test_acc = total_test_acc / steps_per_epoch_test\n",
        "        avg_test_loss = total_test_loss / steps_per_epoch_test\n",
        "\n",
        "        #Display details\n",
        "        #print(f\"Test Loss: {avg_test_loss:.4f} Test Accuracy: {avg_test_acc:.4f}\")\n",
        "\n",
        "        return avg_test_loss, avg_test_acc\n",
        "\n",
        "    \"\"\" This function used to translate english word to respective language\"\"\"\n",
        "    def translate(self, word, get_heatmap=False):\n",
        "        \"\"\"\n",
        "        Translate an input word to the target language.\n",
        "\n",
        "        Args:\n",
        "            word: Input word or sentence to translate\n",
        "            get_heatmap: Whether to return attention weights for visualization\n",
        "\n",
        "        Returns:\n",
        "            result: Translated text\n",
        "            att_wts: Attention weights (if get_heatmap=True)\n",
        "        \"\"\"\n",
        "        # Add start and end tokens for input word\n",
        "        start = \"\\t\"\n",
        "        end = \"\\n\"\n",
        "        word = start + word + end\n",
        "\n",
        "        # Tokenize and pad input\n",
        "        inputs = self.input_tokenizer.texts_to_sequences([word])\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            inputs,\n",
        "            maxlen=self.max_input_len,\n",
        "            padding=\"post\"\n",
        "        )\n",
        "\n",
        "        # Initialize result string and attention weights list\n",
        "        result = \"\"\n",
        "        att_wts = []\n",
        "\n",
        "        # Initialize encoder state and run input through encoder\n",
        "        enc_state = self.encoder.initialize_hidden_state(1)\n",
        "        enc_out, enc_state = self.encoder(inputs, enc_state)\n",
        "\n",
        "        # Set initial decoder state to encoder state\n",
        "        dec_state = enc_state\n",
        "\n",
        "        # Start token as first decoder input\n",
        "        dec_input = tf.expand_dims([self.targ_tokenizer.word_index[start]], 1)\n",
        "\n",
        "        # Generate translation one token at a time\n",
        "        for t in range(1, self.max_target_len):\n",
        "            # Get prediction from decoder\n",
        "            preds, dec_state, attention_weights = self.decoder(dec_input, dec_state, enc_out)\n",
        "\n",
        "            # Store attention weights if needed\n",
        "            if get_heatmap and attention_weights is not None:\n",
        "                att_wts.append(attention_weights)\n",
        "\n",
        "            # Get the predicted token ID\n",
        "            predicted_id = tf.argmax(preds, axis=1)\n",
        "\n",
        "            # Convert ID to character/word\n",
        "            next_char = self.targ_tokenizer.index_word.get(predicted_id.numpy().item(), \"<UNK>\")\n",
        "\n",
        "            # Add to result\n",
        "            result += next_char\n",
        "\n",
        "            # Use prediction as next input\n",
        "            dec_input = tf.expand_dims(predicted_id, 1)\n",
        "\n",
        "            # Stop if end token is generated\n",
        "            if next_char == end:\n",
        "                break\n",
        "\n",
        "        # Remove the end token if present\n",
        "        if result.endswith(end):\n",
        "            result = result[:-1]\n",
        "\n",
        "        return result, att_wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.103339Z",
          "iopub.status.busy": "2022-05-13T17:57:08.102802Z",
          "iopub.status.idle": "2022-05-13T17:57:08.113188Z",
          "shell.execute_reply": "2022-05-13T17:57:08.11221Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.103293Z"
        },
        "id": "rT-bKfIIWcES",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_layer(name, units, dropout, return_state=False, return_sequences=False):\n",
        "    \"\"\"\n",
        "    Create a recurrent layer based on the specified cell type.\n",
        "\n",
        "    Args:\n",
        "        name: String, one of 'rnn', 'gru', or 'lstm'\n",
        "        units: Integer, dimensionality of the output space\n",
        "        dropout: Float between 0 and 1, fraction of the units to drop\n",
        "        return_state: Boolean, whether to return the last state\n",
        "        return_sequences: Boolean, whether to return the sequence of outputs\n",
        "\n",
        "    Returns:\n",
        "        A recurrent layer (SimpleRNN, GRU, or LSTM)\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "\n",
        "    if name == \"rnn\":\n",
        "        return layers.SimpleRNN(\n",
        "            units=units,\n",
        "            dropout=dropout,\n",
        "            return_state=return_state,\n",
        "            return_sequences=return_sequences\n",
        "        )\n",
        "    elif name == \"gru\":\n",
        "        return layers.GRU(\n",
        "            units=units,\n",
        "            dropout=dropout,\n",
        "            return_state=return_state,\n",
        "            return_sequences=return_sequences\n",
        "        )\n",
        "    elif name == \"lstm\":\n",
        "        return layers.LSTM(\n",
        "            units=units,\n",
        "            dropout=dropout,\n",
        "            return_state=return_state,\n",
        "            return_sequences=return_sequences\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported RNN cell type: {name}. Use 'rnn', 'gru', or 'lstm'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **wandb**"
      ],
      "metadata": {
        "id": "aC_ETEF9rIxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.115727Z",
          "iopub.status.busy": "2022-05-13T17:57:08.115041Z",
          "iopub.status.idle": "2022-05-13T17:57:08.125087Z",
          "shell.execute_reply": "2022-05-13T17:57:08.124108Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.115665Z"
        },
        "id": "6dS2JhY05vMR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "execution": {
          "iopub.execute_input": "2022-05-13T17:57:08.12831Z",
          "iopub.status.busy": "2022-05-13T17:57:08.12697Z",
          "iopub.status.idle": "2022-05-13T17:58:08.041134Z",
          "shell.execute_reply": "2022-05-13T17:58:08.039989Z",
          "shell.execute_reply.started": "2022-05-13T17:57:08.128261Z"
        },
        "id": "hLEZUp3W5wf9",
        "outputId": "9a63da22-b628-481e-ee47-1287978922d5",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mna21b050\u001b[0m (\u001b[33mna21b050-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_131220-xumre7ki</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/na21b050-iit-madras/uncategorized/runs/xumre7ki' target=\"_blank\">flowing-dew-10</a></strong> to <a href='https://wandb.ai/na21b050-iit-madras/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/na21b050-iit-madras/uncategorized' target=\"_blank\">https://wandb.ai/na21b050-iit-madras/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/na21b050-iit-madras/uncategorized/runs/xumre7ki' target=\"_blank\">https://wandb.ai/na21b050-iit-madras/uncategorized/runs/xumre7ki</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/na21b050-iit-madras/uncategorized/runs/xumre7ki?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7d89c1b87dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCVhhqPMWkuS"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:08.044395Z",
          "iopub.status.busy": "2022-05-13T17:58:08.044039Z",
          "iopub.status.idle": "2022-05-13T17:58:09.136596Z",
          "shell.execute_reply": "2022-05-13T17:58:09.135546Z",
          "shell.execute_reply.started": "2022-05-13T17:58:08.044346Z"
        },
        "id": "p5Bt3Bf-_JhQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "START_TOKEN=\"\\t\"\n",
        "END_TOKEN=\"\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWI9X48FjCPy",
        "outputId": "9b00c7ea-f382-4c6d-faf7-e33da06e37ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading....\n",
            "download Complete\n",
            "Extracting..\n",
            "Complete\n",
            "You are all set\n"
          ]
        }
      ],
      "source": [
        "downloadDataSet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:09.13871Z",
          "iopub.status.busy": "2022-05-13T17:58:09.138399Z",
          "iopub.status.idle": "2022-05-13T17:58:10.220961Z",
          "shell.execute_reply": "2022-05-13T17:58:10.219885Z",
          "shell.execute_reply.started": "2022-05-13T17:58:09.138662Z"
        },
        "id": "6ji0OxT0yxRA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "language=\"te\"\n",
        "train_dir, val_dir, test_dir = get_files(language)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:10.22341Z",
          "iopub.status.busy": "2022-05-13T17:58:10.222458Z",
          "iopub.status.idle": "2022-05-13T17:58:17.153887Z",
          "shell.execute_reply": "2022-05-13T17:58:17.152529Z",
          "shell.execute_reply.started": "2022-05-13T17:58:10.223364Z"
        },
        "id": "vWOhUaVWipOU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)\n",
        "val_dataset, _, _ = preprocess_data(val_dir,input_tokenizer,targ_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOIv0WNAipOU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:17.156262Z",
          "iopub.status.busy": "2022-05-13T17:58:17.155897Z",
          "iopub.status.idle": "2022-05-13T17:58:20.361771Z",
          "shell.execute_reply": "2022-05-13T17:58:20.360733Z",
          "shell.execute_reply.started": "2022-05-13T17:58:17.156216Z"
        },
        "id": "A57aPT6P6uyg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "dataset, input_tokenizer, targ_tokenizer = preprocess_data(train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-13T17:58:40.546067Z",
          "iopub.status.busy": "2022-05-13T17:58:40.545721Z",
          "iopub.status.idle": "2022-05-13T18:01:06.858305Z",
          "shell.execute_reply": "2022-05-13T18:01:06.854753Z",
          "shell.execute_reply.started": "2022-05-13T17:58:40.546033Z"
        },
        "id": "KZwjhjH0ipOV",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbXxZXZjpEq"
      },
      "source": [
        "## Sweep Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.748889Z",
          "iopub.status.idle": "2022-05-13T17:58:21.749784Z",
          "shell.execute_reply": "2022-05-13T17:58:21.749484Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.749451Z"
        },
        "id": "ikX_8MejzT7-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"DL_Assignment3_Rnn\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\n",
        "      \"name\": \"val_acc\",\n",
        "      \"goal\": \"maximize\",\n",
        "  },\n",
        "\n",
        "  \"parameters\": {\n",
        "        \"num_of_encoders\":{\n",
        "          \"values\":[1,2,3]\n",
        "        },\n",
        "        \"num_of_decoders\":{\n",
        "            \"values\":[1,2,3]\n",
        "\n",
        "        },\n",
        "        \"cell_type\":{\n",
        "          \"values\":['gru']\n",
        "        },\n",
        "\n",
        "        \"lr\":{\n",
        "          \"values\":[0.001,0.005]\n",
        "        },\n",
        "        \"optimizer\":{\n",
        "          \"values\":['adam','rmsprop']\n",
        "        },\n",
        "        \"dropout\":{ \"values\": [0.3,0.5]},\n",
        "        \"latent_dim\":{ \"values\": [128,256,512]},\n",
        "        \"inp_emb_size\": {\"values\": [64,128,256]},\n",
        "\n",
        "        \"batch_size\":{\"values\":[32,64,128]},\n",
        "\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.751564Z",
          "iopub.status.idle": "2022-05-13T17:58:21.752441Z",
          "shell.execute_reply": "2022-05-13T17:58:21.752161Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.752127Z"
        },
        "id": "2dt9LTQAzZHh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This is the main function to use to train/fine-tune the model using wandb runs\n",
        "def train_wandb():\n",
        "    run = wandb.init()\n",
        "\n",
        "    config=wandb.config\n",
        "    # Set the run name\n",
        "    name=\"num_of_encoders(\"+ str(config[\"num_of_encoders\"]) + \")_\"\n",
        "    name = \" num_of_decoders(\" + str(config[\"num_of_decoders\"]) + \")_\"\n",
        "    name += \" cell_type(\" + str(config[\"cell_type\"]) + \")_\"\n",
        "\n",
        "    name += \"latent_dim(\" + str(config[\"latent_dim\"])+ \")_\"\n",
        "    name += \"lr(\" + str(config[\"lr\"])+ \")_\"\n",
        "    name += \"optimizer(\" + str(config[\"optimizer\"]) + \")_\"\n",
        "    name += \"dropout(\" + str(config[\"dropout\"]) + \")\"\n",
        "    name += \"inp_emb_size(\" + str(config[\"inp_emb_size\"]) + \")_\"\n",
        "    name+=\"batch_size(\" + str(config[\"batch_size\"]) + \")\"\n",
        "\n",
        "\n",
        "    wandb.run.name = name[:-1]\n",
        "    batch_size=config[\"batch_size\"]\n",
        "    inp_emb_size=config[\"inp_emb_size\"]\n",
        "    dropout=config[\"dropout\"]\n",
        "    optimizer=config[\"optimizer\"]\n",
        "    num_of_encoders=config[\"num_of_encoders\"]\n",
        "    num_of_decoders=config[\"num_of_decoders\"]\n",
        "\n",
        "    lr=config[\"lr\"]\n",
        "    latent_dim=config[\"latent_dim\"]\n",
        "    cell_type=config[\"cell_type\"]\n",
        "\n",
        "\n",
        "    param=Parameters(language=\"te\",\\\n",
        "                        embedding_dim=inp_emb_size,\\\n",
        "                        encoder_layers=num_of_encoders,\\\n",
        "                        decoder_layers=num_of_decoders,\\\n",
        "                        layer_type=cell_type,\\\n",
        "                        units=latent_dim,\\\n",
        "                        dropout=dropout,\n",
        "                        epochs=15,\\\n",
        "                 batch_size=batch_size\\\n",
        "                   )\n",
        "    param.apply_teacher_forcing=True\n",
        "    param.teacher_forcing_ratio=1\n",
        "    param.patience=5\n",
        "    param.attention=True\n",
        "    model = SequenceTOSequence(param)\n",
        "    model.set_vocabulary(input_tokenizer, targ_tokenizer)\n",
        "\n",
        "    model.build(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\\\n",
        "                metric = tf.keras.metrics.SparseCategoricalAccuracy(),\\\n",
        "                optimizer = optimizer,\\\n",
        "                lr=lr\\\n",
        "                )\n",
        "\n",
        "    model.fit(dataset, val_dataset, epochs=param.epochs, wandb=wandb,teacher_forcing_ratio=param.teacher_forcing_ratio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.754176Z",
          "iopub.status.idle": "2022-05-13T17:58:21.755107Z",
          "shell.execute_reply": "2022-05-13T17:58:21.754787Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.754754Z"
        },
        "id": "kzkdsWRlZzTf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"na21b050-iit-madras\", project=\"DA6401_Assignment3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-13T17:58:21.756822Z",
          "iopub.status.idle": "2022-05-13T17:58:21.757667Z",
          "shell.execute_reply": "2022-05-13T17:58:21.757398Z",
          "shell.execute_reply.started": "2022-05-13T17:58:21.757366Z"
        },
        "id": "LGxzAfZ42R_u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "wandb.agent(sweep_id, train_wandb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}